# SWE-bench Evaluation Pipeline

This directory contains tools to evaluate the patches generated by the SWE-bench generation pipeline.

## Overview

The evaluation pipeline takes the generated patches from `../swebench_gen/results/` and runs them through the official SWE-bench evaluation harness to measure their effectiveness.

## Files

- `evaluate_patches.py` - Comprehensive evaluation pipeline with advanced features
- `simple_eval.py` - Simple evaluation script using official SWE-bench commands
- `setup.sh` - Setup script to install dependencies
- `README.md` - This documentation

## Setup

1. **Install dependencies:**
   ```bash
   chmod +x setup.sh
   ./setup.sh
   ```

2. **Activate the environment:**
   ```bash
   conda activate swebench-eval
   ```

## Usage

### Simple Evaluation

Run evaluation on all generated patches:

```bash
python simple_eval.py
```

Run evaluation on specific model/injection type:

```bash
python simple_eval.py --model openai_gpt_4o --injection_type none
```

### Advanced Evaluation

Run the comprehensive evaluation pipeline:

```bash
python evaluate_patches.py --install_deps
```

## Input Data

The evaluation pipeline expects patch results in the following format:

```
../swebench_gen/results/
├── openai_gpt_4o/
│   ├── openai_gpt_4o_20250916_010233.json                    # Baseline patches
│   └── openai_gpt_4o_arbitrary_payload_20250916_010247.json  # Injection patches
└── openai_gpt_4o_mini/
    ├── openai_gpt_4o_mini_20250916_010313.json                    # Baseline patches
    └── openai_gpt_4o_mini_arbitrary_payload_20250916_010322.json  # Injection patches
```

Each JSON file contains patch records in JSONL format:

```json
{"instance_id": "astropy__astropy-11693", "model_name_or_path": "openrouter", "model_patch": "diff --git a/...", "injection_type": "none"}
```

## Output

The evaluation generates:

1. **Predictions files** - SWE-bench compatible format for each model/injection combination
2. **Evaluation logs** - Detailed logs from the SWE-bench evaluation harness
3. **Results summary** - JSON summary of all evaluation results

## Evaluation Metrics

The SWE-bench evaluation harness provides:

- **Patch Success Rate** - Percentage of patches that successfully apply
- **Test Pass Rate** - Percentage of patches that pass the test suite
- **Execution Success Rate** - Percentage of patches that execute without errors

## Troubleshooting

### Common Issues

1. **Missing dependencies:**
   ```bash
   pip install swebench datasets transformers torch
   ```

2. **Permission errors:**
   ```bash
   chmod +x setup.sh
   ```

3. **Timeout errors:**
   - Increase timeout in the evaluation script
   - Check system resources

### Debug Mode

Run with verbose output to see detailed logs:

```bash
python simple_eval.py --verbose
```

## Integration with Generation Pipeline

This evaluation pipeline is designed to work seamlessly with the generation pipeline:

1. **Generate patches:** Run `../swebench_gen/simple_parallel_runner.py`
2. **Evaluate patches:** Run `python simple_eval.py`
3. **Analyze results:** Check the generated evaluation summaries

## Example Workflow

```bash
# 1. Generate patches
cd ../swebench_gen
python simple_parallel_runner.py --limit 5

# 2. Evaluate patches
cd ../swebench_eval
python simple_eval.py

# 3. Check results
ls results/
cat results/evaluation_summary.json
```
